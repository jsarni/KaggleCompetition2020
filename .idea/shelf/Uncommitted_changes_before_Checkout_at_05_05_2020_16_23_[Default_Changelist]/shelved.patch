Index: DatasetReader/DatasetReader.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pathlib\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport random\r\nimport re\r\nfrom PIL import Image,ImageFile\r\nimport glob\r\nimport pickle\r\n\r\n\r\n\r\nfrom vars import *\r\n\r\nImageFile.LOAD_TRUNCATED_IMAGES = True\r\n\r\n\r\ndef split_dataset():\r\n    print('> Started splitting dataset to train and test batches')\r\n    train_batches_list = [i for i in range(NB_BATCHES)]\r\n    random.shuffle(train_batches_list)\r\n    nb_test_batches = int(0.2 * NB_BATCHES)\r\n    test_batch_list = []\r\n    for i in range(nb_test_batches):\r\n        selected_batch = random.choice(train_batches_list)\r\n        test_batch_list.append(selected_batch)\r\n        train_batches_list.remove(selected_batch)\r\n\r\n    return train_batches_list, test_batch_list\r\n\r\n\r\ndef read_image(image_id):\r\n    image_path_root = '{}{}.*'.format(IMAGES_DIR, image_id)\r\n    image_path = glob.glob(image_path_root)[0]\r\n\r\n    image = Image.open(image_path)\r\n    image = image.resize((IMAGE_HEIGHT, IMAGE_LENGTH))\r\n    image_np = np.array(image)\r\n    return image_np\r\n\r\n\r\ndef read_batch_images(batch_number):\r\n    print('> Started loading batch images')\r\n    batch_path = '{}{}{}'.format(BATCHES_DIR, BATCHES_FILES_ROOT, batch_number)\r\n    batch = pd.read_csv(batch_path)\r\n\r\n    images_ids = batch['image_id'].to_list()\r\n\r\n    images_list = []\r\n\r\n    for image in images_ids:\r\n        images_list.append(read_image(image))\r\n\r\n    return np.array(images_list)\r\n\r\n\r\ndef int_list_from_str(string):\r\n    clean_string = re.sub(r'[\\[\\] ]', \"\", string)\r\n    return [int(i) for i in clean_string.split(',')]\r\n\r\n\r\ndef read_batch_labels(batch_number):\r\n    print('> Started loading batch labels')\r\n    batch_path = '{}{}{}'.format(BATCHES_DIR, BATCHES_FILES_ROOT, batch_number)\r\n    batch = pd.read_csv(batch_path)\r\n\r\n    labels_str = batch['category_id'].to_list()\r\n\r\n    labels = np.array([[int(label_str)] for label_str in labels_str])\r\n    return labels\r\n\r\n\r\ndef split_batch_to_train_and_val(images, labels):\r\n    print('> Started batch to train and validation data')\r\n    nb_val_samples = int(0.2 * len(images)) + 1\r\n    starting_pos = random.randint(0, len(images) - nb_val_samples)\r\n\r\n    val_images = images[starting_pos:(starting_pos + nb_val_samples)]\r\n    val_labels = labels[starting_pos:(starting_pos + nb_val_samples)]\r\n\r\n    train_labels = np.concatenate((labels[:starting_pos, :], labels[(starting_pos + nb_val_samples):, :]))\r\n    train_images = np.concatenate((images[:starting_pos, :], images[(starting_pos + nb_val_samples):, :]))\r\n\r\n    return train_images, train_labels, val_images, val_labels\r\n\r\n\r\ndef load_unsplitted_batch(batch_number):\r\n    batch_images = read_batch_images(batch_number)\r\n    batch_labels = read_batch_labels(batch_number)\r\n\r\n    return batch_images, batch_labels\r\n\r\n\r\ndef load_splitted_batch(batch_number):\r\n    batch_images, batch_labels = load_unsplitted_batch(batch_number)\r\n    return split_batch_to_train_and_val(batch_images, batch_labels)\r\n\r\ndef read_image_to_pickle(image_id):\r\n    image_path_root = '{}{}.*'.format(IMAGES_DIR, image_id)\r\n    image_path = glob.glob(image_path_root)[0]\r\n\r\n    pathlib.Path(PICKELED_IMAGES_DIR).mkdir(parents=True, exist_ok=True)\r\n    pathlib.Path(PICKELED_IMAGES_DIR + '16\\\\').mkdir(parents=True, exist_ok=True)\r\n    pathlib.Path(PICKELED_IMAGES_DIR + '32\\\\').mkdir(parents=True, exist_ok=True)\r\n    pathlib.Path(PICKELED_IMAGES_DIR + '64\\\\').mkdir(parents=True, exist_ok=True)\r\n    pathlib.Path(PICKELED_IMAGES_DIR + '128\\\\').mkdir(parents=True, exist_ok=True)\r\n    pathlib.Path(PICKELED_IMAGES_DIR + '256\\\\').mkdir(parents=True, exist_ok=True)\r\n\r\n\r\n    image = Image.open(image_path)\r\n\r\n    for new_size in [(256, 256), (128, 128), (64, 64), (32, 32), (16, 16)]:\r\n        current_size_dir = new_size[0]\r\n        new_pickeled_image_path = '{}{}\\\\{}'.format(PICKELED_IMAGES_DIR, current_size_dir, image_id)\r\n        r_image = image.resize(new_size)\r\n        r_image_np = np.array(r_image)\r\n        flattened_image = r_image_np.flatten()\r\n\r\n        with open(new_pickeled_image_path, 'wb') as pickled_image:\r\n            pickle.dump(flattened_image, pickled_image)\r\n\r\ndef read_batch_images_to_pickle(batch_number):\r\n    batch_path = '{}{}{}'.format(BATCHES_DIR, BATCHES_FILES_ROOT, batch_number)\r\n    batch = pd.read_csv(batch_path)\r\n\r\n    images_ids = batch['image_id'].to_list()\r\n\r\n    nb_images = 1\r\n    total_images = len(images_ids)\r\n    for image in images_ids:\r\n        print(\"=====> image {} / {}\".format(nb_images, total_images))\r\n        read_image_to_pickle(image)\r\n        nb_images += 1\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- DatasetReader/DatasetReader.py	(revision 876fc2590ca7e5fae4e0daae446a3d989a46328b)
+++ DatasetReader/DatasetReader.py	(date 1588688429827)
@@ -124,6 +124,7 @@
     batch = pd.read_csv(batch_path)
 
     images_ids = batch['image_id'].to_list()
+    images_ids = ['8fd080e6-21bc-11ea-a13a-137349068a90']
 
     nb_images = 1
     total_images = len(images_ids)
